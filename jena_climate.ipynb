{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6d9465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69e31446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.7.1+cu118\n",
      "Numpy version: 1.23.5\n",
      "Pandas version: 1.5.3\n",
      "Using device: cuda\n",
      "GPU Model: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "CUDA Version: 11.8\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import random\n",
    "\n",
    "# 设置随机种子以确保结果的可复现性\n",
    "torch.manual_seed(42)  # 设置 PyTorch 的随机种子\n",
    "np.random.seed(42)     # 设置 NumPy 的随机种子\n",
    "random.seed(42)        # 设置 Python 内置随机模块的种子\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "\n",
    "\n",
    "\n",
    "# 检测并设置设备（CPU 或 GPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 打印设备信息\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 如果使用的是 GPU，打印 GPU 的详细信息\n",
    "if device.type == \"cuda\":\n",
    "    print(\"GPU Model:\", torch.cuda.get_device_name(0))  # 打印 GPU 模型\n",
    "    print(\"CUDA Version:\", torch.version.cuda)          # 打印 CUDA 版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f343263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已成功解压到 jena_climate\n",
      "成功加载 CSV 文件，数据行数: 420551\n",
      "选择的特征列: ['T (degC)', 'rh (%)', 'wv (m/s)']\n",
      "处理后的数据已保存为 weather.csv\n",
      "特征列 T (degC) 归一化完成，归一化器已保存为 T (degC)_scaler.pkl\n",
      "特征列 rh (%) 归一化完成，归一化器已保存为 rh (%)_scaler.pkl\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wv (m/s)_scaler.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 106\u001b[0m\n\u001b[0;32m    104\u001b[0m zip_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjena_climate.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m extract_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjena_climate\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 106\u001b[0m \u001b[43mpreprocess_weather_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 69\u001b[0m, in \u001b[0;36mpreprocess_weather_data\u001b[1;34m(zip_path, extract_path, features, sequence_length, target_column)\u001b[0m\n\u001b[0;32m     67\u001b[0m     data_scaled[:, i] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(col_values)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     68\u001b[0m     scalers[col] \u001b[38;5;241m=\u001b[39m scaler\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcol\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_scaler.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m特征列 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 归一化完成，归一化器已保存为 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_scaler.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# 6.1 合并单列 scaler 成一个多列 scaler\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dell\\.conda\\envs\\pytorch_118_python310_jupyter\\lib\\site-packages\\joblib\\numpy_pickle.py:599\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol)\u001b[0m\n\u001b[0;32m    597\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    600\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wv (m/s)_scaler.pkl'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "该脚本的主要功能是处理气象数据\n",
    "包括解压数据文件、加载数据、预处理、归一化和构造时间序列样本。\n",
    "具体步骤如下：\n",
    "1. 解压 ZIP 文件，提取其中的 CSV 数据文件。\n",
    "2. 加载 CSV 文件，读取气象数据。\n",
    "3. 将日期时间列转换为 datetime 类型，便于后续处理。\n",
    "4. 选择需要的气象特征列，并去除缺失值。\n",
    "5. 将处理后的数据保存为新的 CSV 文件（可选）。\n",
    "6. 对特征数据进行归一化处理，以便更好地训练模型。\n",
    "7. 构造时间序列样本，将数据划分为输入序列 X 和目标标签 y。\n",
    "8. 输出样本输入 X 和标签 y 的维度信息，确保数据格式正确。\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import joblib\n",
    "import re\n",
    "def safe_filename(name):\n",
    "    return re.sub(r'[^a-zA-Z0-9_]', '_', name)\n",
    "\n",
    "# 定义一个函数来处理气象数据\n",
    "def preprocess_weather_data(zip_path, extract_path, features=None, sequence_length=72, target_column=\"T (degC)\"):\n",
    "    \"\"\"\n",
    "    处理气象数据的函数\n",
    "    :param zip_path: ZIP 文件路径\n",
    "    :param extract_path: 解压路径\n",
    "    :param features: 需要处理的特征列，默认为 [\"T (degC)\", \"rh (%)\", \"wv (m/s)\"]\n",
    "    :param sequence_length: 时间序列的长度，默认为 72\n",
    "    :param target_column: 预测目标列，默认为 \"T (degC)\"\n",
    "    \"\"\"\n",
    "    # 如果没有指定特征列，则使用默认值\n",
    "    if features is None:\n",
    "        features = [\"T (degC)\", \"rh (%)\", \"wv (m/s)\"]\n",
    "    \n",
    "    # ========== 1. 解压 ZIP 文件 ==========\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(f\"文件已成功解压到 {extract_path}\")\n",
    "    \n",
    "    # ========== 2. 加载 CSV 数据 ==========\n",
    "    csv_path = os.path.join(extract_path, \"jena_climate_2009_2016.csv\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"成功加载 CSV 文件，数据行数: {len(df)}\")\n",
    "    \n",
    "    # ========== 3. 日期转换 ==========\n",
    "    df[\"Date Time\"] = pd.to_datetime(df[\"Date Time\"])\n",
    "    \n",
    "    # ========== 4. 选择特征列 ==========\n",
    "    # 检查指定的特征列是否存在于数据集中\n",
    "    missing_features = [f for f in features if f not in df.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"以下特征列不存在于数据集中: {missing_features}\")\n",
    "    \n",
    "    df_selected = df[[\"Date Time\"] + features].dropna()\n",
    "    print(f\"选择的特征列: {features}\")\n",
    "    \n",
    "    # ========== 5. 保存为新的 CSV 文件 ==========\n",
    "    df_selected.to_csv(\"weather.csv\", index=False)\n",
    "    print(\"处理后的数据已保存为 weather.csv\")\n",
    "    \n",
    "    # 6. 每列单独归一化\n",
    "    scalers = {}\n",
    "    data_scaled = np.zeros_like(df_selected[features].values, dtype=np.float32)\n",
    "    for i, col in enumerate(features):\n",
    "        scaler = MinMaxScaler()\n",
    "        col_values = df_selected[[col]].values\n",
    "        data_scaled[:, i] = scaler.fit_transform(col_values).flatten()\n",
    "        scalers[col] = scaler\n",
    "        joblib.dump(scaler, f\"{safe_filename(col)}_scaler.pkl\")\n",
    "        print(f\"特征列 {col} 归一化完成，归一化器已保存为 {col}_scaler.pkl\")\n",
    "    \n",
    "    # 6.1 合并单列 scaler 成一个多列 scaler\n",
    "    merged_scaler = MinMaxScaler()\n",
    "    merged_scaler.min_ = np.array([scalers[col].min_ for col in features]).flatten()\n",
    "    merged_scaler.scale_ = np.array([scalers[col].scale_ for col in features]).flatten()\n",
    "    merged_scaler.data_min_ = np.array([scalers[col].data_min_ for col in features]).flatten()\n",
    "    merged_scaler.data_max_ = np.array([scalers[col].data_max_ for col in features]).flatten()\n",
    "    merged_scaler.data_range_ = np.array([scalers[col].data_range_ for col in features]).flatten()\n",
    "    merged_scaler.n_features_in_ = len(features)\n",
    "    joblib.dump(merged_scaler, \"merged_scaler.pkl\")\n",
    "    print(\"合并后的归一化器已保存为 merged_scaler.pkl\")\n",
    "    \n",
    "    # 7. 构造时间序列样本\n",
    "    def create_sequences(data, sequence_length, target_column_index):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - sequence_length):\n",
    "            X.append(data[i:i + sequence_length])\n",
    "            y.append(data[i + sequence_length][target_column_index])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    target_column_index = features.index(target_column)\n",
    "    X, y = create_sequences(data_scaled, sequence_length, target_column_index)\n",
    "    \n",
    "    # 8. 输出维度信息\n",
    "    print(\"样本输入 X shape:\", X.shape)\n",
    "    print(\"标签 y shape:\", y.shape)\n",
    "    \n",
    "    # 保存处理后的数据\n",
    "    np.save(\"X.npy\", X)\n",
    "    np.save(\"y.npy\", y)\n",
    "    print(\"处理后的数据已保存为 X.npy 和 y.npy\")\n",
    "\n",
    "# 调用示例\n",
    "zip_path = \"jena_climate.zip\"\n",
    "extract_path = \"jena_climate\"\n",
    "preprocess_weather_data(zip_path, extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b59a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\"\"\"\n",
    "该代码的功能是将气象数据封装为 PyTorch 数据集，并划分训练集和验证集。\n",
    "主要步骤：\n",
    "1. 定义一个继承自 Dataset 的 WeatherDataset 类，用于封装气象数据。\n",
    "2. 创建数据集实例，并根据指定比例随机划分训练集和验证集。\n",
    "3. 创建训练集和验证集的数据加载器 DataLoader 用于批量加载数据。\n",
    "\"\"\"\n",
    "\n",
    "# 定义一个天气数据集类，继承自 torch.utils.data.Dataset\n",
    "class WeatherDataset(Dataset):\n",
    "    \"\"\"\n",
    "    天气数据集类，用于封装气象数据。\n",
    "    :param X: 特征数据\n",
    "    :param y: 目标数据\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        # 初始化函数，将输入的特征数据 X 和目标数据 y 转换为 torch 张量\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  # 特征数据\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # 以确保输出形状为 (batch, 1)，方便回归模型处理\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集的长度，即样本数量。\n",
    "        \"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        根据索引 idx 获取数据集中的一个样本，返回特征和目标。\n",
    "        :param idx: 样本索引\n",
    "        :return: 特征和目标\n",
    "        \"\"\"\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def create_data_loaders(X, y, train_ratio=0.8, batch_size=32, shuffle_train=True, shuffle_val=False):\n",
    "    \"\"\"\n",
    "    创建训练集和验证集的数据加载器。\n",
    "    :param X: 特征数据\n",
    "    :param y: 目标数据\n",
    "    :param train_ratio: 训练集占总数据集的比例，默认为 0.8\n",
    "    :param batch_size: 每个批次的样本数量，默认为 32\n",
    "    :param shuffle_train: 是否打乱训练集，默认为 True\n",
    "    :param shuffle_val: 是否打乱验证集，默认为 False\n",
    "    :return: 训练集数据加载器和验证集数据加载器\n",
    "    \"\"\"\n",
    "    # 创建数据集实例\n",
    "    dataset = WeatherDataset(X, y)\n",
    "\n",
    "    # 计算训练集和验证集的大小\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "    # 随机划分数据集为训练集和验证集\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # 创建训练集和验证集的数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_train)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle_val)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# 示例：使用封装的函数创建数据加载器\n",
    "# 假设 X 和 y 分别是特征数据和目标数据（需要提前定义）\n",
    "# train_loader, val_loader = create_data_loaders(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef9253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === 参数汇总 ===\n",
    "# 输入特征维度: input_dim\n",
    "# 隐藏层维度: hidden_dim (默认值: 64)\n",
    "# 激素模块隐藏层维度: hormone_dim (默认值: 32)\n",
    "# 最大序列长度: max_seq_len (默认值: 72)\n",
    "# 池化策略: pooling (默认值: \"mean\"，可选值: \"mean\", \"max\", \"last\", \"attention\")\n",
    "# 图注意力层多头注意力头数: n_heads (默认值: 8)\n",
    "# Dropout 概率: dropout (默认值: 0.1)\n",
    "# Transformer 编码器前馈网络维度: dim_feedforward (默认值: hidden_dim * 4)\n",
    "# Transformer 编码器层数: num_layers (默认值: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2154d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    图注意力层，用于模拟节点之间的间接影响。\n",
    "    参数：\n",
    "        in_dim (int): 输入特征的维度\n",
    "        out_dim (int): 输出特征的维度\n",
    "        n_heads (int): 多头注意力的头数，默认为 8\n",
    "        dropout (float): Dropout 概率，默认为 0.1\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, n_heads=8, dropout=0.1):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.n_heads = n_heads  # 多头注意力的头数\n",
    "        self.out_dim = out_dim  # 输出维度\n",
    "        self.W = nn.Linear(in_dim, out_dim, bias=False)  # 线性变换，将输入维度映射到输出维度\n",
    "        self.a = nn.Parameter(torch.empty(size=(2 * (out_dim // n_heads), 1)))  # 注意力参数矩阵\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)  # 使用 Xavier 初始化方法初始化注意力参数矩阵\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout 层，用于防止过拟合\n",
    "        \n",
    "        # 如果输入维度和输出维度不一致，则使用残差投影层\n",
    "        if in_dim != out_dim:\n",
    "            self.residual_proj = nn.Linear(in_dim, out_dim)\n",
    "        else:\n",
    "            self.residual_proj = nn.Identity()\n",
    "\n",
    "    def forward(self, x, adj_matrix=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        参数：\n",
    "            x (torch.Tensor): 输入特征，形状为 [B, T, D]\n",
    "            adj_matrix (torch.Tensor, optional): 动态邻接矩阵，形状为 [B, T, T]，默认为 None\n",
    "        返回：\n",
    "            torch.Tensor: 输出特征，形状为 [B, T, out_dim]\n",
    "        \"\"\"\n",
    "        B, T, D = x.size()  # 输入张量的形状，B 是批次大小，T 是节点数量，D 是输入特征维度\n",
    "        h = self.W(x).view(B, T, self.n_heads, -1)  # 应用线性变换并调整形状\n",
    "        h = h.permute(0, 2, 1, 3)  # 调整维度顺序，方便后续操作\n",
    "\n",
    "        # 生成节点特征的重复版本，用于计算节点之间的注意力权重\n",
    "        h_repeat = h.unsqueeze(3).repeat(1, 1, 1, T, 1)  # [B, n_heads, T, T, H//n_heads]\n",
    "        h_repeat_T = h.unsqueeze(2).repeat(1, 1, T, 1, 1)  # [B, n_heads, T, T, H//n_heads]\n",
    "\n",
    "        # 将重复的特征拼接起来，用于计算注意力权重\n",
    "        combined = torch.cat([h_repeat, h_repeat_T], dim=-1)  # [B, n_heads, T, T, 2*H//n_heads]\n",
    "\n",
    "        # 调整形状以方便矩阵乘法\n",
    "        combined_reshape = combined.reshape(B * self.n_heads * T * T, -1)  # 展平为二维张量\n",
    "        e = torch.matmul(combined_reshape, self.a).squeeze(-1)  # 计算注意力权重的原始值\n",
    "        e = e.view(B, self.n_heads, T, T)  # 恢复原始形状\n",
    "\n",
    "        # 应用 Softmax 函数，将注意力权重归一化为概率分布\n",
    "        alpha = torch.softmax(e, dim=-1)  # [B, n_heads, T, T]\n",
    "        alpha = self.dropout(alpha)  # 应用 Dropout\n",
    "        \n",
    "        # 如果提供了动态邻接矩阵，则应用掩码\n",
    "        if adj_matrix is not None:\n",
    "            mask = (adj_matrix == 0).unsqueeze(1)  # [B, 1, T, T]\n",
    "            e = e.masked_fill(mask, float('-inf'))  # 注意力分数先掩码\n",
    "        alpha = torch.softmax(e, dim=-1)  # 应用 Softmax 函数\n",
    "        alpha = self.dropout(alpha)  # 应用 Dropout\n",
    "        # 使用注意力权重加权求和，得到每个节点的输出特征\n",
    "        out = torch.matmul(alpha, h)  # [B, n_heads, T, H//n_heads]\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(B, T, self.out_dim)  # 调整形状并合并多头特征\n",
    "        return out + self.residual_proj(x)  # 应用残差连接\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e17c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 激素记忆模块 ===\n",
    "class HormoneModule(nn.Module):\n",
    "    \"\"\"\n",
    "    激素记忆模块，用于模拟激素对信息融合的调控作用。\n",
    "    参数：\n",
    "        input_dim (int): 输入特征的维度\n",
    "        hidden_dim (int): GRU 隐藏层维度（记忆）\n",
    "        output_dim (int): 控制信号输出的目标维度\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(HormoneModule, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)  # GRU 层\n",
    "        self.controller = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),  # 线性层\n",
    "            nn.ReLU(),  # ReLU 激活函数\n",
    "            nn.Linear(hidden_dim, 2 * output_dim)  # 输出层，生成 alpha 和 beta 控制信号\n",
    "        )\n",
    "\n",
    "    def forward(self, x, prev_state=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        参数：\n",
    "            x (torch.Tensor): 输入特征，形状为 [B, T, D]\n",
    "            prev_state (torch.Tensor, optional): 上一时刻的激素状态，形状为 [1, B, hidden_dim]，默认为 None\n",
    "        返回：\n",
    "            alpha (torch.Tensor): 控制信号 alpha，形状为 [B, output_dim]\n",
    "            beta (torch.Tensor): 控制信号 beta，形状为 [B, output_dim]\n",
    "            hormone_state (torch.Tensor): 当前时刻的激素状态，形状为 [1, B, hidden_dim]\n",
    "        \"\"\"\n",
    "        _, h = self.gru(x, prev_state)  # h: [1, B, hidden_dim]\n",
    "        h = h.squeeze(0)  # [B, hidden_dim]\n",
    "        gate = F.softplus(self.controller(h))  # 使用 Softplus 激活函数，使输出为正值\n",
    "\n",
    "        alpha, beta = gate.chunk(2, dim=-1)  # 将输出分为 alpha 和 beta 两部分\n",
    "        gate_sum = torch.clamp(alpha + beta, min=1e-8)  # 防止除零\n",
    "        alpha = alpha / gate_sum  # 归一化 alpha\n",
    "        beta = beta / gate_sum  # 归一化 beta\n",
    "        return alpha, beta, h.unsqueeze(0)  # 返回控制信号和激素状态\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5add1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 时间编码器（多尺度 Transformer 层） ===\n",
    "class TemporalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    时间编码器，用于建模时间序列信息。\n",
    "    参数：\n",
    "        input_dim (int): 输入特征的维度\n",
    "        n_heads (int): 注意力头的数量，默认为 4\n",
    "        dropout (float): Dropout 概率，默认为 0.1\n",
    "        max_seq_len (int): 最大序列长度，默认为 512\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, n_heads=4, dropout=0.1, max_seq_len=512):\n",
    "        super(TemporalEncoder, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim,  # 输入特征维度\n",
    "            nhead=n_heads,  # 注意力头数量\n",
    "            dim_feedforward=input_dim * 4,  # 前馈网络的维度\n",
    "            dropout=dropout,  # Dropout 概率\n",
    "            batch_first=True  # 批次维度在前\n",
    "        )\n",
    "        self.pos_enc = nn.Parameter(torch.randn(max_seq_len, input_dim))  # 位置编码参数\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout 层\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        参数：\n",
    "            x (torch.Tensor): 输入特征，形状为 [B, T, D]\n",
    "            mask (torch.Tensor, optional): 掩码，用于处理不同长度的序列，默认为 None\n",
    "        返回：\n",
    "            torch.Tensor: 编码后的特征，形状为 [B, T, D]\n",
    "        \"\"\"\n",
    "        x = x + self.pos_enc[:x.size(1), :].unsqueeze(0)  # 添加位置编码\n",
    "        x = self.dropout(x)  # 应用 Dropout\n",
    "        return self.encoder(x, src_key_padding_mask=mask) + x  # 使用 Transformer 编码器处理输入，并添加残差连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f58d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 最终的脑启发式模型 ===\n",
    "class BrainInspiredNetV2(nn.Module):\n",
    "    \"\"\"\n",
    "    脑启发式模型，结合直接路径、间接路径、激素调控和时间编码。\n",
    "    参数：\n",
    "        input_dim (int): 输入特征的维度\n",
    "        hidden_dim (int): 隐藏层的维度，默认为 64\n",
    "        hormone_dim (int): 激素模块的隐藏层维度，默认为 32\n",
    "        max_seq_len (int): 最大序列长度，默认为 72\n",
    "        pooling (str): 序列汇聚策略，可选 'mean', 'max', 'last', 'attention'，默认为 'mean'\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=64, hormone_dim=32, max_seq_len=72, pooling=\"mean\"):\n",
    "        super(BrainInspiredNetV2, self).__init__()\n",
    "        # ① 直接路径 MLP\n",
    "        self.direct_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),  # 线性层\n",
    "            nn.ReLU(),  # ReLU 激活函数\n",
    "            nn.LayerNorm(hidden_dim),  # 层归一化\n",
    "            nn.Dropout(0.2),  # Dropout 层\n",
    "            nn.Linear(hidden_dim, hidden_dim)  # 线性层\n",
    "        )\n",
    "\n",
    "        # ② 间接路径 GAT\n",
    "        self.indirect_path = GraphAttentionLayer(input_dim, hidden_dim, n_heads=8)\n",
    "\n",
    "        # ③ 激素调控模块\n",
    "        self.hormone_mod = HormoneModule(input_dim, hormone_dim, hidden_dim)\n",
    "\n",
    "        # ④ 时间编码器（Transformer）\n",
    "        self.temporal_encoder = TemporalEncoder(hidden_dim, max_seq_len=max_seq_len)\n",
    "\n",
    "        # ⑤ 融合 + 输出\n",
    "        self.fusion_dim = nn.Linear(hidden_dim, hidden_dim)  # 融合层\n",
    "        self.out_layer = nn.Linear(hidden_dim, 1)  # 输出层\n",
    "\n",
    "        # 显式增加输出对齐层（用于 direct/indirect 路径）\n",
    "        self.direct_proj = nn.Identity()  # 若 direct_out 已是 hidden_dim 可省略转换\n",
    "        self.indirect_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # 新增汇聚策略\n",
    "        self.pooling = pooling\n",
    "        if pooling == \"attention\":\n",
    "            self.att_pool = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, 128),  # 线性层\n",
    "                nn.Tanh(),  # Tanh 激活函数\n",
    "                nn.Linear(128, 1)  # 线性层\n",
    "            )\n",
    "\n",
    "    def forward(self, x, adj_matrix=None, hormone_prev=None, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        参数：\n",
    "            x (torch.Tensor): 输入特征，形状为 [B, T, D]\n",
    "            adj_matrix (torch.Tensor, optional): 动态邻接矩阵，形状为 [B, T, T]，默认为 None\n",
    "            hormone_prev (torch.Tensor, optional): 上一时刻的激素状态，形状为 [1, B, hidden_dim]，默认为 None\n",
    "            mask (torch.Tensor, optional): 掩码，用于处理不同长度的序列，默认为 None\n",
    "        返回：\n",
    "            torch.Tensor: 最终预测结果，形状为 [B]\n",
    "            torch.Tensor: 当前时刻的激素状态，形状为 [1, B, hidden_dim]\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "\n",
    "        # ① 路径输出\n",
    "        direct_out = self.direct_mlp(x)  # [B, T, H]\n",
    "        indirect_out = self.indirect_path(x, adj_matrix)  # [B, T, H]\n",
    "\n",
    "        # ② 显式维度对齐\n",
    "        direct_out = self.direct_proj(direct_out)  # [B, T, H]\n",
    "        indirect_out = self.indirect_proj(indirect_out)  # [B, T, H]\n",
    "\n",
    "        # ③ 激素调控融合\n",
    "        alpha, beta, hormone_state = self.hormone_mod(x, hormone_prev)  # [B, H]\n",
    "        alpha = alpha.unsqueeze(1)  # [B, 1, H]\n",
    "        beta = beta.unsqueeze(1)  # [B, 1, H]\n",
    "        fused = alpha * direct_out + beta * indirect_out  # [B, T, H]\n",
    "\n",
    "        # ④ 时间编码器\n",
    "        fused = self.fusion_dim(fused)  # [B, T, H]\n",
    "        encoded = self.temporal_encoder(fused, mask=mask)  # [B, T, H]\n",
    "\n",
    "        # ⑤ 最终输出（池化 + 回归）\n",
    "        # 根据 pooling 选择不同的汇聚方式\n",
    "        if self.pooling == \"mean\":\n",
    "            pooled = encoded.mean(dim=1)  # 平均池化\n",
    "        elif self.pooling == \"max\":\n",
    "            pooled, _ = encoded.max(dim=1)  # 最大池化\n",
    "        elif self.pooling == \"last\":\n",
    "            pooled = encoded[:, -1, :]  # 取最后时间步\n",
    "        elif self.pooling == \"attention\":\n",
    "            att_weights = self.att_pool(encoded).squeeze(-1)  # [B, T]\n",
    "            if mask is not None:\n",
    "                att_weights = att_weights.masked_fill(mask, float('-inf'))\n",
    "            att_weights = torch.softmax(att_weights, dim=1).unsqueeze(-1)  # [B, T, 1]\n",
    "            pooled = (encoded * att_weights).sum(dim=1)  # 加权池化\n",
    "        else:\n",
    "            raise ValueError(f\"未知的池化类型: {self.pooling}\")\n",
    "\n",
    "        y = self.out_layer(pooled)  # [B, 1]\n",
    "        return y.squeeze(-1), hormone_state  # [B], [1, B, H]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5248a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除激素调控模块 Ablation NoHormone\n",
    "class BrainInspiredNetV2_NoHormone(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, max_seq_len=72, pooling=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.direct_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.indirect_path = GraphAttentionLayer(input_dim, hidden_dim, n_heads=8)\n",
    "        self.temporal_encoder = TemporalEncoder(hidden_dim, max_seq_len=max_seq_len)\n",
    "        self.fusion_dim = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.direct_proj = nn.Identity()\n",
    "        self.indirect_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.pooling = pooling\n",
    "        if pooling == \"attention\":\n",
    "            self.att_pool = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, 128),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x, adj_matrix=None, hormone_prev=None, mask=None):\n",
    "        B, T, D = x.shape\n",
    "        direct_out = self.direct_mlp(x)\n",
    "        indirect_out = self.indirect_path(x, adj_matrix)\n",
    "        direct_out = self.direct_proj(direct_out)\n",
    "        indirect_out = self.indirect_proj(indirect_out)\n",
    "        # 无激素调控，直接简单融合（平均）\n",
    "        fused = 0.5 * direct_out + 0.5 * indirect_out\n",
    "        fused = self.fusion_dim(fused)\n",
    "        encoded = self.temporal_encoder(fused, mask=mask)\n",
    "        if self.pooling == \"mean\":\n",
    "            pooled = encoded.mean(dim=1)\n",
    "        elif self.pooling == \"max\":\n",
    "            pooled, _ = encoded.max(dim=1)\n",
    "        elif self.pooling == \"last\":\n",
    "            pooled = encoded[:, -1, :]\n",
    "        elif self.pooling == \"attention\":\n",
    "            att_weights = self.att_pool(encoded).squeeze(-1)\n",
    "            if mask is not None:\n",
    "                att_weights = att_weights.masked_fill(mask, float('-inf'))\n",
    "            att_weights = torch.softmax(att_weights, dim=1).unsqueeze(-1)\n",
    "            pooled = (encoded * att_weights).sum(dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"未知的池化类型: {self.pooling}\")\n",
    "        y = self.out_layer(pooled)\n",
    "        return y.squeeze(-1), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a26125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除间接路径（GraphAttentionLayer） Ablation NoIndirect\n",
    "class BrainInspiredNetV2_NoIndirect(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, hormone_dim=32, max_seq_len=72, pooling=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.direct_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.hormone_mod = HormoneModule(input_dim, hormone_dim, hidden_dim)\n",
    "        self.temporal_encoder = TemporalEncoder(hidden_dim, max_seq_len=max_seq_len)\n",
    "        self.fusion_dim = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.direct_proj = nn.Identity()\n",
    "        self.pooling = pooling\n",
    "        if pooling == \"attention\":\n",
    "            self.att_pool = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, 128),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x, adj_matrix=None, hormone_prev=None, mask=None):\n",
    "        B, T, D = x.shape\n",
    "        direct_out = self.direct_mlp(x)\n",
    "        direct_out = self.direct_proj(direct_out)\n",
    "        alpha, beta, hormone_state = self.hormone_mod(x, hormone_prev)\n",
    "        alpha = alpha.unsqueeze(1)\n",
    "        beta = beta.unsqueeze(1)\n",
    "        # 没有 indirect 路径，用 alpha 调节直接路径，beta 失效\n",
    "        fused = alpha * direct_out  # 只保留直接路径调节\n",
    "        fused = self.fusion_dim(fused)\n",
    "        encoded = self.temporal_encoder(fused, mask=mask)\n",
    "        if self.pooling == \"mean\":\n",
    "            pooled = encoded.mean(dim=1)\n",
    "        elif self.pooling == \"max\":\n",
    "            pooled, _ = encoded.max(dim=1)\n",
    "        elif self.pooling == \"last\":\n",
    "            pooled = encoded[:, -1, :]\n",
    "        elif self.pooling == \"attention\":\n",
    "            att_weights = self.att_pool(encoded).squeeze(-1)\n",
    "            if mask is not None:\n",
    "                att_weights = att_weights.masked_fill(mask, float('-inf'))\n",
    "            att_weights = torch.softmax(att_weights, dim=1).unsqueeze(-1)\n",
    "            pooled = (encoded * att_weights).sum(dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"未知的池化类型: {self.pooling}\")\n",
    "        y = self.out_layer(pooled)\n",
    "        return y.squeeze(-1), hormone_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ba42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除时间编码器 Ablation NoTemporalEncoder\n",
    "class BrainInspiredNetV2_NoTemporal(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, hormone_dim=32, max_seq_len=72, pooling=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.direct_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.indirect_path = GraphAttentionLayer(input_dim, hidden_dim, n_heads=8)\n",
    "        self.hormone_mod = HormoneModule(input_dim, hormone_dim, hidden_dim)\n",
    "        self.fusion_dim = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.direct_proj = nn.Identity()\n",
    "        self.indirect_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.pooling = pooling\n",
    "        if pooling == \"attention\":\n",
    "            self.att_pool = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, 128),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x, adj_matrix=None, hormone_prev=None, mask=None):\n",
    "        B, T, D = x.shape\n",
    "        direct_out = self.direct_mlp(x)\n",
    "        indirect_out = self.indirect_path(x, adj_matrix)\n",
    "        direct_out = self.direct_proj(direct_out)\n",
    "        indirect_out = self.indirect_proj(indirect_out)\n",
    "        alpha, beta, hormone_state = self.hormone_mod(x, hormone_prev)\n",
    "        alpha = alpha.unsqueeze(1)\n",
    "        beta = beta.unsqueeze(1)\n",
    "        fused = alpha * direct_out + beta * indirect_out\n",
    "        fused = self.fusion_dim(fused)\n",
    "        # 跳过 temporal_encoder 直接池化\n",
    "        if self.pooling == \"mean\":\n",
    "            pooled = fused.mean(dim=1)\n",
    "        elif self.pooling == \"max\":\n",
    "            pooled, _ = fused.max(dim=1)\n",
    "        elif self.pooling == \"last\":\n",
    "            pooled = fused[:, -1, :]\n",
    "        elif self.pooling == \"attention\":\n",
    "            att_weights = self.att_pool(fused).squeeze(-1)\n",
    "            if mask is not None:\n",
    "                att_weights = att_weights.masked_fill(mask, float('-inf'))\n",
    "            att_weights = torch.softmax(att_weights, dim=1).unsqueeze(-1)\n",
    "            pooled = (fused * att_weights).sum(dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"未知的池化类型: {self.pooling}\")\n",
    "        y = self.out_layer(pooled)\n",
    "        return y.squeeze(-1), hormone_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00c4fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#只保留简单全连接（极简版本）\n",
    "class BrainInspiredNetV2_SimpleFC(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    def forward(self, x, adj_matrix=None, hormone_prev=None, mask=None):\n",
    "        # 输入形状 [B, T, D]，先简单取平均时间步\n",
    "        x = x.mean(dim=1)\n",
    "        y = self.fc(x)\n",
    "        return y.squeeze(-1), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ff479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 可视化模块 ===\n",
    "def visualize_alpha_beta(alpha, beta, batch_idx=0, title=\"Alpha/Beta Weights\", save_path=None):\n",
    "    \"\"\"\n",
    "    可视化 alpha 和 beta 权重的变化。\n",
    "    参数：\n",
    "        alpha (torch.Tensor): alpha 权重，形状为 [B, T, H]\n",
    "        beta (torch.Tensor): beta 权重，形状为 [B, T, H]\n",
    "        batch_idx (int): 选择要可视化的批次，默认为 0\n",
    "        title (str): 图表标题，默认为 \"Alpha/Beta Weights\"\n",
    "    \"\"\"\n",
    "    B, T, H = alpha.shape\n",
    "    alpha = alpha[batch_idx].detach().cpu().numpy()\n",
    "    beta = beta[batch_idx].detach().cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(H):\n",
    "        plt.subplot(H, 1, i + 1)\n",
    "        plt.plot(alpha[:, i], label=f\"Alpha {i+1}\")\n",
    "        plt.plot(beta[:, i], label=f\"Beta {i+1}\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"Weights for Head {i+1}\")\n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.ylabel(\"Weight Value\")\n",
    "    plt.suptitle(title)\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "'''\n",
    "# === 使用示例 ===\n",
    "if __name__ == \"__main__\":\n",
    "    # 示例输入数据\n",
    "    B, T, D = 2, 10, 10  # 批次大小、时间步长、特征维度\n",
    "    x = torch.randn(B, T, D)  # 示例输入\n",
    "    adj_matrix = torch.randint(0, 2, (B, T, T)).float()  # 示例动态邻接矩阵\n",
    "\n",
    "    # 初始化模型\n",
    "    model = BrainInspiredNetV2(input_dim=D, hidden_dim=64, hormone_dim=32, max_seq_len=T, pooling=\"mean\")\n",
    "\n",
    "    # 前向传播\n",
    "    y, hormone_state = model(x, adj_matrix=adj_matrix)\n",
    "\n",
    "    # 可视化 alpha 和 beta 权重\n",
    "    alpha, beta, _ = model.hormone_mod(x)\n",
    "    visualize_alpha_beta(alpha, beta)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055e66b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # === 参数汇总 ===\n",
    "# # 输入特征维度: input_dim\n",
    "# # 隐藏层维度: hidden_dim (默认值: 64)\n",
    "# # 激素模块隐藏层维度: hormone_dim (默认值: 32)\n",
    "# # 最大序列长度: max_seq_len (默认值: 72)\n",
    "# # 池化策略: pooling (默认值: \"mean\"，可选值: \"mean\", \"max\", \"last\", \"attention\")\n",
    "# # 图注意力层多头注意力头数: n_heads (默认值: 8)\n",
    "# # Dropout 概率: dropout (默认值: 0.1)\n",
    "# # Transformer 编码器前馈网络维度: dim_feedforward (默认值: hidden_dim * 4)\n",
    "# # Transformer 编码器层数: num_layers (默认值: 1)\n",
    "\n",
    "# # === 间接影响的图注意力层 ===\n",
    "# class GraphAttentionLayer(nn.Module):\n",
    "#     \"\"\"\n",
    "#     图注意力层，用于模拟节点之间的间接影响。\n",
    "#     参数：\n",
    "#         in_dim (int): 输入特征的维度\n",
    "#         out_dim (int): 输出特征的维度\n",
    "#         n_heads (int): 多头注意力的头数，默认为 8\n",
    "#         dropout (float): Dropout 概率，默认为 0.1\n",
    "#     \"\"\"\n",
    "#     def __init__(self, in_dim, out_dim, n_heads=8, dropout=0.1):\n",
    "#         super(GraphAttentionLayer, self).__init__()\n",
    "#         self.n_heads = n_heads  # 多头注意力的头数\n",
    "#         self.out_dim = out_dim  # 输出维度\n",
    "#         self.W = nn.Linear(in_dim, out_dim, bias=False)  # 线性变换，将输入维度映射到输出维度\n",
    "#         self.a = nn.Parameter(torch.empty(size=(2 * (out_dim // n_heads), 1)))  # 注意力参数矩阵\n",
    "#         nn.init.xavier_uniform_(self.a.data, gain=1.414)  # 使用 Xavier 初始化方法初始化注意力参数矩阵\n",
    "#         self.dropout = nn.Dropout(dropout)  # Dropout 层，用于防止过拟合\n",
    "        \n",
    "#         # 如果输入维度和输出维度不一致，则使用残差投影层\n",
    "#         if in_dim != out_dim:\n",
    "#             self.residual_proj = nn.Linear(in_dim, out_dim)\n",
    "#         else:\n",
    "#             self.residual_proj = nn.Identity()\n",
    "\n",
    "#     def forward(self, x, adj_matrix=None):\n",
    "#         \"\"\"\n",
    "#         前向传播函数。\n",
    "#         参数：\n",
    "#             x (torch.Tensor): 输入特征，形状为 [B, T, D]\n",
    "#             adj_matrix (torch.Tensor, optional): 动态邻接矩阵，形状为 [B, T, T]，默认为 None\n",
    "#         返回：\n",
    "#             torch.Tensor: 输出特征，形状为 [B, T, out_dim]\n",
    "#         \"\"\"\n",
    "#         B, T, D = x.size()  # 输入张量的形状，B 是批次大小，T 是节点数量，D 是输入特征维度\n",
    "#         h = self.W(x).view(B, T, self.n_heads, -1)  # 应用线性变换并调整形状\n",
    "#         h = h.permute(0, 2, 1, 3)  # 调整维度顺序，方便后续操作\n",
    "\n",
    "#         # 生成节点特征的重复版本，用于计算节点之间的注意力权重\n",
    "#         h_repeat = h.unsqueeze(3).repeat(1, 1, 1, T, 1)  # [B, n_heads, T, T, H//n_heads]\n",
    "#         h_repeat_T = h.unsqueeze(2).repeat(1, 1, T, 1, 1)  # [B, n_heads, T, T, H//n_heads]\n",
    "\n",
    "#         # 将重复的特征拼接起来，用于计算注意力权重\n",
    "#         combined = torch.cat([h_repeat, h_repeat_T], dim=-1)  # [B, n_heads, T, T, 2*H//n_heads]\n",
    "\n",
    "#         # 调整形状以方便矩阵乘法\n",
    "#         combined_reshape = combined.reshape(B * self.n_heads * T * T, -1)  # 展平为二维张量\n",
    "#         e = torch.matmul(combined_reshape, self.a).squeeze(-1)  # 计算注意力权重的原始值\n",
    "#         e = e.view(B, self.n_heads, T, T)  # 恢复原始形状\n",
    "\n",
    "#         # 应用 Softmax 函数，将注意力权重归一化为概率分布\n",
    "#         alpha = torch.softmax(e, dim=-1)  # [B, n_heads, T, T]\n",
    "#         alpha = self.dropout(alpha)  # 应用 Dropout\n",
    "        \n",
    "#         # 如果提供了动态邻接矩阵，则应用掩码\n",
    "#         if adj_matrix is not None:\n",
    "#             mask = (adj_matrix == 0).unsqueeze(1)  # [B, 1, T, T]\n",
    "#             e = e.masked_fill(mask, float('-inf'))  # 注意力分数先掩码\n",
    "#         alpha = torch.softmax(e, dim=-1)  # 应用 Softmax 函数\n",
    "#         alpha = self.dropout(alpha)  # 应用 Dropout\n",
    "#         # 使用注意力权重加权求和，得到每个节点的输出特征\n",
    "#         out = torch.matmul(alpha, h)  # [B, n_heads, T, H//n_heads]\n",
    "#         out = out.permute(0, 2, 1, 3).contiguous().view(B, T, self.out_dim)  # 调整形状并合并多头特征\n",
    "#         return out + self.residual_proj(x)  # 应用残差连接\n",
    "\n",
    "\n",
    "# # === 激素记忆模块 ===\n",
    "# class HormoneModule(nn.Module):\n",
    "#     \"\"\"\n",
    "#     激素记忆模块，用于模拟激素对信息融合的调控作用。\n",
    "#     参数：\n",
    "#         input_dim (int): 输入特征的维度\n",
    "#         hidden_dim (int): GRU 隐藏层维度（记忆）\n",
    "#         output_dim (int): 控制信号输出的目标维度\n",
    "#     \"\"\"\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(HormoneModule, self).__init__()\n",
    "#         self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)  # GRU 层\n",
    "#         self.controller = nn.Sequential(\n",
    "#             nn.Linear(hidden_dim, hidden_dim),  # 线性层\n",
    "#             nn.ReLU(),  # ReLU 激活函数\n",
    "#             nn.Linear(hidden_dim, 2 * output_dim)  # 输出层，生成 alpha 和 beta 控制信号\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, prev_state=None):\n",
    "#         \"\"\"\n",
    "#         前向传播函数。\n",
    "#         参数：\n",
    "#             x (torch.Tensor): 输入特征，形状为 [B, T, D]\n",
    "#             prev_state (torch.Tensor, optional): 上一时刻的激素状态，形状为 [1, B, hidden_dim]，默认为 None\n",
    "#         返回：\n",
    "#             alpha (torch.Tensor): 控制信号 alpha，形状为 [B, output_dim]\n",
    "#             beta (torch.Tensor): 控制信号 beta，形状为 [B, output_dim]\n",
    "#             hormone_state (torch.Tensor): 当前时刻的激素状态，形状为 [1, B, hidden_dim]\n",
    "#         \"\"\"\n",
    "#         _, h = self.gru(x, prev_state)  # h: [1, B, hidden_dim]\n",
    "#         h = h.squeeze(0)  # [B, hidden_dim]\n",
    "#         gate = F.softplus(self.controller(h))  # 使用 Softplus 激活函数，使输出为正值\n",
    "\n",
    "#         alpha, beta = gate.chunk(2, dim=-1)  # 将输出分为 alpha 和 beta 两部分\n",
    "#         gate_sum = torch.clamp(alpha + beta, min=1e-8)  # 防止除零\n",
    "#         alpha = alpha / gate_sum  # 归一化 alpha\n",
    "#         beta = beta / gate_sum  # 归一化 beta\n",
    "#         return alpha, beta, h.unsqueeze(0)  # 返回控制信号和激素状态\n",
    "\n",
    "\n",
    "# # === 时间编码器（多尺度 Transformer 层） ===\n",
    "# class TemporalEncoder(nn.Module):\n",
    "#     \"\"\"\n",
    "#     时间编码器，用于建模时间序列信息。\n",
    "#     参数：\n",
    "#         input_dim (int): 输入特征的维度\n",
    "#         n_heads (int): 注意力头的数量，默认为 4\n",
    "#         dropout (float): Dropout 概率，默认为 0.1\n",
    "#         max_seq_len (int): 最大序列长度，默认为 512\n",
    "#     \"\"\"\n",
    "#     def __init__(self, input_dim, n_heads=4, dropout=0.1, max_seq_len=512):\n",
    "#         super(TemporalEncoder, self).__init__()\n",
    "#         self.encoder = nn.TransformerEncoderLayer(\n",
    "#             d_model=input_dim,  # 输入特征维度\n",
    "#             nhead=n_heads,  # 注意力头数量\n",
    "#             dim_feedforward=input_dim * 4,  # 前馈网络的维度\n",
    "#             dropout=dropout,  # Dropout 概率\n",
    "#             batch_first=True  # 批次维度在前\n",
    "#         )\n",
    "#         self.pos_enc = nn.Parameter(torch.randn(max_seq_len, input_dim))  # 位置编码参数\n",
    "#         self.dropout = nn.Dropout(dropout)  # Dropout 层\n",
    "\n",
    "#     def forward(self, x, mask=None):\n",
    "#         \"\"\"\n",
    "#         前向传播函数。\n",
    "#         参数：\n",
    "#             x (torch.Tensor): 输入特征，形状为 [B, T, D]\n",
    "#             mask (torch.Tensor, optional): 掩码，用于处理不同长度的序列，默认为 None\n",
    "#         返回：\n",
    "#             torch.Tensor: 编码后的特征，形状为 [B, T, D]\n",
    "#         \"\"\"\n",
    "#         x = x + self.pos_enc[:x.size(1), :].unsqueeze(0)  # 添加位置编码\n",
    "#         x = self.dropout(x)  # 应用 Dropout\n",
    "#         return self.encoder(x, src_key_padding_mask=mask) + x  # 使用 Transformer 编码器处理输入，并添加残差连接\n",
    "\n",
    "\n",
    "# # === 最终的脑启发式模型 ===\n",
    "# class BrainInspiredNetV2(nn.Module):\n",
    "#     \"\"\"\n",
    "#     脑启发式模型，结合直接路径、间接路径、激素调控和时间编码。\n",
    "#     参数：\n",
    "#         input_dim (int): 输入特征的维度\n",
    "#         hidden_dim (int): 隐藏层的维度，默认为 64\n",
    "#         hormone_dim (int): 激素模块的隐藏层维度，默认为 32\n",
    "#         max_seq_len (int): 最大序列长度，默认为 72\n",
    "#         pooling (str): 序列汇聚策略，可选 'mean', 'max', 'last', 'attention'，默认为 'mean'\n",
    "#     \"\"\"\n",
    "#     def __init__(self, input_dim, hidden_dim=64, hormone_dim=32, max_seq_len=72, pooling=\"mean\"):\n",
    "#         super(BrainInspiredNetV2, self).__init__()\n",
    "#         # ① 直接路径 MLP\n",
    "#         self.direct_mlp = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),  # 线性层\n",
    "#             nn.ReLU(),  # ReLU 激活函数\n",
    "#             nn.LayerNorm(hidden_dim),  # 层归一化\n",
    "#             nn.Dropout(0.2),  # Dropout 层\n",
    "#             nn.Linear(hidden_dim, hidden_dim)  # 线性层\n",
    "#         )\n",
    "\n",
    "#         # ② 间接路径 GAT\n",
    "#         self.indirect_path = GraphAttentionLayer(input_dim, hidden_dim, n_heads=8)\n",
    "\n",
    "#         # ③ 激素调控模块\n",
    "#         self.hormone_mod = HormoneModule(input_dim, hormone_dim, hidden_dim)\n",
    "\n",
    "#         # ④ 时间编码器（Transformer）\n",
    "#         self.temporal_encoder = TemporalEncoder(hidden_dim, max_seq_len=max_seq_len)\n",
    "\n",
    "#         # ⑤ 融合 + 输出\n",
    "#         self.fusion_dim = nn.Linear(hidden_dim, hidden_dim)  # 融合层\n",
    "#         self.out_layer = nn.Linear(hidden_dim, 1)  # 输出层\n",
    "\n",
    "#         # 显式增加输出对齐层（用于 direct/indirect 路径）\n",
    "#         self.direct_proj = nn.Identity()  # 若 direct_out 已是 hidden_dim 可省略转换\n",
    "#         self.indirect_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "#         # 新增汇聚策略\n",
    "#         self.pooling = pooling\n",
    "#         if pooling == \"attention\":\n",
    "#             self.att_pool = nn.Sequential(\n",
    "#                 nn.Linear(hidden_dim, 128),  # 线性层\n",
    "#                 nn.Tanh(),  # Tanh 激活函数\n",
    "#                 nn.Linear(128, 1)  # 线性层\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x, adj_matrix=None, hormone_prev=None, mask=None):\n",
    "#         \"\"\"\n",
    "#         前向传播函数。\n",
    "#         参数：\n",
    "#             x (torch.Tensor): 输入特征，形状为 [B, T, D]\n",
    "#             adj_matrix (torch.Tensor, optional): 动态邻接矩阵，形状为 [B, T, T]，默认为 None\n",
    "#             hormone_prev (torch.Tensor, optional): 上一时刻的激素状态，形状为 [1, B, hidden_dim]，默认为 None\n",
    "#             mask (torch.Tensor, optional): 掩码，用于处理不同长度的序列，默认为 None\n",
    "#         返回：\n",
    "#             torch.Tensor: 最终预测结果，形状为 [B]\n",
    "#             torch.Tensor: 当前时刻的激素状态，形状为 [1, B, hidden_dim]\n",
    "#         \"\"\"\n",
    "#         B, T, D = x.shape\n",
    "\n",
    "#         # ① 路径输出\n",
    "#         direct_out = self.direct_mlp(x)  # [B, T, H]\n",
    "#         indirect_out = self.indirect_path(x, adj_matrix)  # [B, T, H]\n",
    "\n",
    "#         # ② 显式维度对齐\n",
    "#         direct_out = self.direct_proj(direct_out)  # [B, T, H]\n",
    "#         indirect_out = self.indirect_proj(indirect_out)  # [B, T, H]\n",
    "\n",
    "#         # ③ 激素调控融合\n",
    "#         alpha, beta, hormone_state = self.hormone_mod(x, hormone_prev)  # [B, H]\n",
    "#         alpha = alpha.unsqueeze(1)  # [B, 1, H]\n",
    "#         beta = beta.unsqueeze(1)  # [B, 1, H]\n",
    "#         fused = alpha * direct_out + beta * indirect_out  # [B, T, H]\n",
    "\n",
    "#         # ④ 时间编码器\n",
    "#         fused = self.fusion_dim(fused)  # [B, T, H]\n",
    "#         encoded = self.temporal_encoder(fused, mask=mask)  # [B, T, H]\n",
    "\n",
    "#         # ⑤ 最终输出（池化 + 回归）\n",
    "#         # 根据 pooling 选择不同的汇聚方式\n",
    "#         if self.pooling == \"mean\":\n",
    "#             pooled = encoded.mean(dim=1)  # 平均池化\n",
    "#         elif self.pooling == \"max\":\n",
    "#             pooled, _ = encoded.max(dim=1)  # 最大池化\n",
    "#         elif self.pooling == \"last\":\n",
    "#             pooled = encoded[:, -1, :]  # 取最后时间步\n",
    "#         elif self.pooling == \"attention\":\n",
    "#             att_weights = self.att_pool(encoded).squeeze(-1)  # [B, T]\n",
    "#             if mask is not None:\n",
    "#                 att_weights = att_weights.masked_fill(mask, float('-inf'))\n",
    "#             att_weights = torch.softmax(att_weights, dim=1).unsqueeze(-1)  # [B, T, 1]\n",
    "#             pooled = (encoded * att_weights).sum(dim=1)  # 加权池化\n",
    "#         else:\n",
    "#             raise ValueError(f\"未知的池化类型: {self.pooling}\")\n",
    "\n",
    "#         y = self.out_layer(pooled)  # [B, 1]\n",
    "#         return y.squeeze(-1), hormone_state  # [B], [1, B, H]\n",
    "\n",
    "# # === 可视化模块 ===\n",
    "# def visualize_alpha_beta(alpha, beta, batch_idx=0, title=\"Alpha/Beta Weights\", save_path=None):\n",
    "#     \"\"\"\n",
    "#     可视化 alpha 和 beta 权重的变化。\n",
    "#     参数：\n",
    "#         alpha (torch.Tensor): alpha 权重，形状为 [B, T, H]\n",
    "#         beta (torch.Tensor): beta 权重，形状为 [B, T, H]\n",
    "#         batch_idx (int): 选择要可视化的批次，默认为 0\n",
    "#         title (str): 图表标题，默认为 \"Alpha/Beta Weights\"\n",
    "#     \"\"\"\n",
    "#     B, T, H = alpha.shape\n",
    "#     alpha = alpha[batch_idx].detach().cpu().numpy()\n",
    "#     beta = beta[batch_idx].detach().cpu().numpy()\n",
    "\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     for i in range(H):\n",
    "#         plt.subplot(H, 1, i + 1)\n",
    "#         plt.plot(alpha[:, i], label=f\"Alpha {i+1}\")\n",
    "#         plt.plot(beta[:, i], label=f\"Beta {i+1}\")\n",
    "#         plt.legend()\n",
    "#         plt.title(f\"Weights for Head {i+1}\")\n",
    "#         plt.xlabel(\"Time Step\")\n",
    "#         plt.ylabel(\"Weight Value\")\n",
    "#     plt.suptitle(title)\n",
    "#     if save_path is not None:\n",
    "#         plt.savefig(save_path)\n",
    "#     plt.show()\n",
    "# '''\n",
    "# # === 使用示例 ===\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 示例输入数据\n",
    "#     B, T, D = 2, 10, 10  # 批次大小、时间步长、特征维度\n",
    "#     x = torch.randn(B, T, D)  # 示例输入\n",
    "#     adj_matrix = torch.randint(0, 2, (B, T, T)).float()  # 示例动态邻接矩阵\n",
    "\n",
    "#     # 初始化模型\n",
    "#     model = BrainInspiredNetV2(input_dim=D, hidden_dim=64, hormone_dim=32, max_seq_len=T, pooling=\"mean\")\n",
    "\n",
    "#     # 前向传播\n",
    "#     y, hormone_state = model(x, adj_matrix=adj_matrix)\n",
    "\n",
    "#     # 可视化 alpha 和 beta 权重\n",
    "#     alpha, beta, _ = model.hormone_mod(x)\n",
    "#     visualize_alpha_beta(alpha, beta)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_logger(log_path=None, level=logging.INFO):\n",
    "    \"\"\"\n",
    "    配置日志记录器。支持同时输出到控制台和文件（自动添加时间戳）。\n",
    "\n",
    "    参数:\n",
    "        log_path (str): 日志文件路径前缀（如 logs/train.log），将自动加时间戳。\n",
    "        level (int): 日志级别，默认为 logging.INFO。\n",
    "\n",
    "    返回:\n",
    "        logger (logging.Logger): 配置完成的 logger 对象。\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    # 防止重复初始化 handler\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "    # 控制台输出\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(level)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    if log_path:\n",
    "        log_dir = os.path.dirname(log_path)\n",
    "        if log_dir and not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_path_with_timestamp = f\"{os.path.splitext(log_path)[0]}_{timestamp}{os.path.splitext(log_path)[1]}\"\n",
    "        file_handler = logging.FileHandler(log_path_with_timestamp)\n",
    "        file_handler.setLevel(level)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.info(f\" Logging to file: {log_path_with_timestamp}\")\n",
    "        logger.log_path = log_path_with_timestamp  # 可选：绑定属性用于后续访问\n",
    "\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59567020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "def evaluate_model(model, data_loader, criterion, device, use_mask=False):\n",
    "    \"\"\"\n",
    "    评估模型性能。\n",
    "\n",
    "    参数:\n",
    "        model (torch.nn.Module): 模型。\n",
    "        data_loader (torch.utils.data.DataLoader): 数据加载器。\n",
    "        criterion (callable): 损失函数。\n",
    "        device (str): 设备（\"cpu\" 或 \"cuda\"）。\n",
    "        use_mask (bool): 是否使用 mask 处理缺失点。默认为 False。\n",
    "\n",
    "    返回值:\n",
    "        avg_loss (float): 平均损失。\n",
    "        r2 (float or list of float): 决定系数（R²），如果是多目标则返回列表。\n",
    "        mae (float or list of float): 平均绝对误差（MAE），如果是多目标则返回列表。\n",
    "        rmse (float or list of float): 均方根误差（RMSE），如果是多目标则返回列表。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss_sum = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            pred = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "            # 如果输出是 [batch_size, 1]，转为 [batch_size]\n",
    "            if pred.dim() == 2 and pred.size(1) == 1:\n",
    "                pred = pred.squeeze(1)\n",
    "            if y_batch.dim() == 2 and y_batch.size(1) == 1:\n",
    "                y_batch = y_batch.squeeze(1)\n",
    "            if use_mask:\n",
    "                mask = ~torch.isnan(y_batch)\n",
    "                pred_masked = pred[mask]         # shape: [有效元素数]\n",
    "                y_batch_masked = y_batch[mask]   # shape: [有效元素数]\n",
    "            else:\n",
    "                pred_masked = pred               # shape: [batch_size]\n",
    "                y_batch_masked = y_batch         # shape: [batch_size]\n",
    "\n",
    "\n",
    "            loss = criterion(pred_masked, y_batch_masked)\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            y_true.append(y_batch_masked.cpu().numpy())\n",
    "            y_pred.append(pred_masked.cpu().numpy())\n",
    "\n",
    "    avg_loss = loss_sum / len(data_loader)\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true[:, None]\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred[:, None]\n",
    "\n",
    "\n",
    "    is_multi_target = y_true.shape[-1] > 1\n",
    "    metrics = {\"R2\": [], \"MAE\": [], \"RMSE\": []}\n",
    "\n",
    "    for i in range(y_true.shape[-1]):\n",
    "        y_true_i = y_true[:, i]\n",
    "        y_pred_i = y_pred[:, i]\n",
    "\n",
    "        if len(y_true_i) == 0 or np.all(np.isnan(y_true_i)):\n",
    "            metrics[\"R2\"].append(np.nan)\n",
    "            metrics[\"MAE\"].append(np.nan)\n",
    "            metrics[\"RMSE\"].append(np.nan)\n",
    "        else:\n",
    "            try:\n",
    "                metrics[\"R2\"].append(r2_score(y_true_i, y_pred_i))\n",
    "                metrics[\"MAE\"].append(mean_absolute_error(y_true_i, y_pred_i))\n",
    "                metrics[\"RMSE\"].append(np.sqrt(mean_squared_error(y_true_i, y_pred_i)))\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"计算评估指标时出现异常：{e}\")\n",
    "                metrics[\"R2\"].append(np.nan)\n",
    "                metrics[\"MAE\"].append(np.nan)\n",
    "                metrics[\"RMSE\"].append(np.nan)\n",
    "\n",
    "    if not is_multi_target:\n",
    "        metrics[\"R2\"] = metrics[\"R2\"][0]\n",
    "        metrics[\"MAE\"] = metrics[\"MAE\"][0]\n",
    "        metrics[\"RMSE\"] = metrics[\"RMSE\"][0]\n",
    "\n",
    "    return avg_loss, metrics[\"R2\"], metrics[\"MAE\"], metrics[\"RMSE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c06925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    封装模型训练和验证的完整流程。\n",
    "\n",
    "    参数:\n",
    "        model (torch.nn.Module): 要训练的模型。\n",
    "        train_loader (torch.utils.data.DataLoader): 训练数据加载器。\n",
    "        val_loader (torch.utils.data.DataLoader): 验证数据加载器。\n",
    "        optimizer (torch.optim.Optimizer): 优化器。\n",
    "        criterion (callable): 损失函数。\n",
    "        device (str): 训练设备（\"cpu\" 或 \"cuda\"）。\n",
    "        max_epochs (int): 最大训练轮数。默认为 100。\n",
    "        patience (int): 早停机制的耐心值。默认为 10。\n",
    "        lr_scheduler_patience (int): 学习率调度器的耐心值。默认为 3。\n",
    "        lr_scheduler_factor (float): 学习率调度器的衰减因子。默认为 0.5。\n",
    "        save_path (str): 模型保存路径。默认为 None。\n",
    "        log_path (str): 日志文件路径。默认为 None。\n",
    "        custom_callbacks (list): 自定义回调函数列表。默认为 None。\n",
    "        use_mask (bool): 是否使用 mask 处理缺失点。默认为 False。\n",
    "        save_state_dict (bool): 是否保存模型的 state_dict 而非完整模型对象。默认为 False。\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_loader, val_loader, optimizer, criterion, device, \n",
    "                 max_epochs=100, patience=10, lr_scheduler_patience=3, \n",
    "                 lr_scheduler_factor=0.5, save_path=None, log_path=None, \n",
    "                 custom_callbacks=None, use_mask=False, save_state_dict=False):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.lr_scheduler_patience = lr_scheduler_patience\n",
    "        self.lr_scheduler_factor = lr_scheduler_factor\n",
    "        self.save_path = save_path\n",
    "        self.log_path = log_path\n",
    "        self.custom_callbacks = custom_callbacks if custom_callbacks else []\n",
    "        self.use_mask = use_mask\n",
    "        self.save_state_dict = save_state_dict\n",
    "\n",
    "        # 配置日志记录器\n",
    "        setup_logger(log_path)\n",
    "\n",
    "        # 检查设备是否可用\n",
    "        if device == \"cuda\" and not torch.cuda.is_available():\n",
    "            logger.warning(\"CUDA不可用，切换到CPU\")\n",
    "            self.device = \"cpu\"\n",
    "        self.device = torch.device(self.device)\n",
    "\n",
    "        # 将模型移动到指定设备\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        开始训练模型。\n",
    "\n",
    "        返回值:\n",
    "            dict: 包含训练过程中的损失、指标和最佳模型信息。\n",
    "        \"\"\"\n",
    "        best_loss = float(\"inf\")\n",
    "        best_model_state = None\n",
    "        best_epoch = 0\n",
    "        train_losses, val_losses = [], []\n",
    "        val_metrics = {\"R2\": [], \"MAE\": [], \"RMSE\": []}\n",
    "\n",
    "        # 初始化学习率调度器\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, patience=self.lr_scheduler_patience, factor=self.lr_scheduler_factor\n",
    "        )\n",
    "        early_stop_counter = 0\n",
    "\n",
    "        # 如果保存路径包含目录且目录不存在，则创建目录\n",
    "        if self.save_path:\n",
    "            dir_name = os.path.dirname(self.save_path)\n",
    "            if dir_name and not os.path.exists(dir_name):\n",
    "                os.makedirs(dir_name)\n",
    "\n",
    "        total_start_time = time.time()\n",
    "        for epoch in range(1, self.max_epochs + 1):\n",
    "            epoch_start_time = time.time()\n",
    "            self.model.train()\n",
    "            train_loss_sum = 0.0\n",
    "\n",
    "            if len(self.train_loader) == 0:\n",
    "                logger.error(\"训练数据加载器为空\")\n",
    "                raise ValueError(\"训练数据加载器为空\")\n",
    "\n",
    "            # 遍历训练数据加载器中的每个批次\n",
    "            for batch in tqdm(self.train_loader, desc=f\"Epoch {epoch:03d} Training\", leave=True):\n",
    "                x_batch, y_batch = batch\n",
    "                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
    "\n",
    "                # 使用中间变量统一预测维度\n",
    "                #pred_aligned = pred.view(-1, 1)  # 将 [batch_size] 转为 [batch_size, 1]\n",
    "                # 如果使用 mask 处理缺失值\n",
    "                #if self.use_mask:\n",
    "                #    mask = ~torch.isnan(y_batch)  # 创建 mask，标记非缺失值\n",
    "                #    #y_batch_masked = y_batch[mask].reshape(-1, y_batch.size(-1))  # 使用 reshape 替代 view\n",
    "                #    y_batch_masked = y_batch[mask].view(-1)\n",
    "                #    pred_masked = pred[mask].view(-1)\n",
    "                #else:\n",
    "                #    #y_batch_masked = y_batch\n",
    "                #    y_batch_masked = y_batch.view(-1)\n",
    "                #    pred_masked = pred.view(-1)\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                \n",
    "                # === 模型前向传播 ===\n",
    "                outputs = self.model(x_batch)\n",
    "\n",
    "                if isinstance(outputs, tuple):\n",
    "                    \n",
    "                    #print(f\"[DEBUG] model output is tuple with length {len(outputs)}\")\n",
    "                    #print(f\"[DEBUG] first element shape: {outputs[0].shape}\")\n",
    "                    pred = outputs[0]  # 原始模型输出\n",
    "                else:\n",
    "                    #print(f\"[DEBUG] model output shape: {outputs.shape}\")\n",
    "                    pred = outputs\n",
    "                    \n",
    "                # 如果输出形状是 [batch_size, 1]，则 squeeze 成 [batch_size]\n",
    "                if pred.dim() == 2 and pred.size(1) == 1:\n",
    "                    pred = pred.squeeze(1)  # [32, 1] -> [32]\n",
    "                # === 统一预测维度为 [batch_size, 1] ===\n",
    "                #pred_aligned = pred_raw.view(-1, 1)\n",
    "                \n",
    "                # 标签也同样处理，确保是 [batch_size]\n",
    "                if y_batch.dim() == 2 and y_batch.size(1) == 1:\n",
    "                    y_batch = y_batch.squeeze(1)  # [32, 1] -> [32]\n",
    "                # === Mask处理和标签统一维度 ===\n",
    "                if self.use_mask:\n",
    "                # 构造mask：标记非缺失值\n",
    "                    mask = ~torch.isnan(y_batch)\n",
    "\n",
    "                    # 将标签和预测都压缩到1D用于计算损失\n",
    "                    pred_masked = pred[mask]\n",
    "                    y_batch_masked = y_batch[mask]\n",
    "                else:\n",
    "                    #y_batch_masked = y_batch.view(-1)            # [batch_size]\n",
    "                    #pred_masked = pred.view(-1)          # [batch_size]\n",
    "                    #y_batch_masked = y_batch.squeeze(-1)  # 将 [batch_size, 1] -> [batch_size]\n",
    "                    #pred_masked = pred.squeeze(-1)        # 同上，确保一致\n",
    "                    y_batch_masked = y_batch.view(-1)  # shape: [batch_size, 1]\n",
    "                    pred_masked = pred.view(-1)         # shape: [batch_size, 1]\n",
    "\n",
    "                # === 损失计算 ===\n",
    "                #print(f\"[DEBUG] pred_masked shape: {pred_masked.shape}, y_batch_masked shape: {y_batch_masked.shape}\")\n",
    "                #print(f\"pred_masked shape: {pred_masked.shape}, y_batch_masked shape: {y_batch_masked.shape}\")\n",
    "\n",
    "                loss = self.criterion(pred_masked, y_batch_masked)\n",
    "\n",
    "                \n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss_sum += loss.item()\n",
    "\n",
    "            # 计算平均训练损失\n",
    "            avg_train_loss = train_loss_sum / len(self.train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "\n",
    "            # 在验证集上评估模型\n",
    "            avg_val_loss, r2, mae, rmse = evaluate_model(self.model, self.val_loader, self.criterion, self.device, self.use_mask)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_metrics[\"R2\"].append(r2)\n",
    "            val_metrics[\"MAE\"].append(mae)\n",
    "            val_metrics[\"RMSE\"].append(rmse)\n",
    "\n",
    "            # 调整学习率\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            # 计算当前轮次的耗时\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_duration = epoch_end_time - epoch_start_time\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            # 记录日志\n",
    "            log_data = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": float(avg_train_loss) if not isinstance(avg_train_loss, float) else avg_train_loss,  # 确保是原生 float\n",
    "                \"val_loss\": float(avg_val_loss) if not isinstance(avg_val_loss, float) else avg_val_loss,  # 确保是原生 float\n",
    "                \"R2\": float(r2) if not isinstance(r2, float) else r2,  # 确保是原生 float\n",
    "                \"MAE\": float(mae) if not isinstance(mae, float) else mae,  # 确保是原生 float\n",
    "                \"RMSE\": float(rmse) if not isinstance(rmse, float) else rmse,  # 确保是原生 float\n",
    "                \"lr\": float(current_lr) if not isinstance(current_lr, float) else current_lr,  # 确保是原生 float\n",
    "                \"duration\": float(epoch_duration) if not isinstance(epoch_duration, float) else epoch_duration,  # 确保是原生 float\n",
    "                \"samples_per_second\": float(len(self.train_loader.dataset) / epoch_duration)  # 确保是原生 float\n",
    "            }\n",
    "            logger.info(json.dumps(log_data))\n",
    "\n",
    "\n",
    "            # 如果当前轮次的验证损失是最佳的，则保存模型状态\n",
    "            if avg_val_loss < best_loss:\n",
    "                best_loss = avg_val_loss\n",
    "                best_model_state = self.model.state_dict()\n",
    "                best_epoch = epoch\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                # 如果早停计数器达到耐心值，则触发早停机制\n",
    "                if early_stop_counter >= self.patience:\n",
    "                    logger.info(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "            # 调用自定义回调函数\n",
    "            if self.custom_callbacks:\n",
    "                for callback in self.custom_callbacks:\n",
    "                    try:\n",
    "                        callback(self.model, epoch, avg_train_loss, avg_val_loss, self.optimizer, scheduler, best_loss, best_epoch)\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Callback {callback} raised an exception: {e}\")\n",
    "\n",
    "        # 计算总训练时间\n",
    "        total_end_time = time.time()\n",
    "        total_duration = total_end_time - total_start_time\n",
    "        logger.info(f\"Training completed in {total_duration/60:.2f} minutes\")\n",
    "\n",
    "        # 加载最佳模型状态\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "            # 保存模型\n",
    "            if self.save_path:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                save_path_with_timestamp = f\"{os.path.splitext(self.save_path)[0]}_{timestamp}{os.path.splitext(self.save_path)[1]}\"\n",
    "                if self.save_state_dict:\n",
    "                    torch.save(best_model_state, save_path_with_timestamp)  # 保存 state_dict\n",
    "                else:\n",
    "                    torch.save(self.model, save_path_with_timestamp)  # 保存完整模型对象\n",
    "                logger.info(f\"Best model saved to {save_path_with_timestamp}\")\n",
    "\n",
    "        # 打印并记录最佳 epoch 的评估结果\n",
    "        best_metrics = {\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"best_loss\": best_loss,\n",
    "            \"best_R2\": val_metrics[\"R2\"][best_epoch - 1],\n",
    "            \"best_MAE\": val_metrics[\"MAE\"][best_epoch - 1],\n",
    "            \"best_RMSE\": val_metrics[\"RMSE\"][best_epoch - 1]\n",
    "        }\n",
    "        logger.info(f\"Best validation metrics at epoch {best_epoch}: {best_metrics}\")\n",
    "\n",
    "        return {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"val_metrics\": val_metrics,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"best_loss\": best_loss,\n",
    "            \"total_duration\": total_duration\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc22e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)  # 建议模块级 logger\n",
    "\n",
    "\n",
    "def save_intermediate_model(model, epoch, train_loss, val_loss, optimizer=None, scheduler=None, best_loss=None, best_epoch=None):\n",
    "    # 函数实现\n",
    "    \"\"\"\n",
    "    自定义回调函数：保存中间模型及优化器状态。\n",
    "\n",
    "    参数:\n",
    "        model (torch.nn.Module): 当前模型。\n",
    "        epoch (int): 当前轮数。\n",
    "        train_loss (float): 当前轮数的训练损失。\n",
    "        val_loss (float): 当前轮数的验证损失。\n",
    "        optimizer (torch.optim.Optimizer): 当前优化器（可选）。\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler): 当前调度器（可选）。\n",
    "    \n",
    "    返回:\n",
    "        str: 保存的 checkpoint 路径。\n",
    "    \"\"\"\n",
    "    dir_path = \"models\"\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_path = os.path.join(dir_path, f\"intermediate_epoch_{epoch}_{timestamp}.pth\")\n",
    "\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "    }\n",
    "\n",
    "    if optimizer:\n",
    "        checkpoint[\"optimizer_state_dict\"] = optimizer.state_dict()\n",
    "        lr = optimizer.param_groups[0].get(\"lr\", None)\n",
    "        if lr is not None:\n",
    "            logger.info(f\"[Epoch {epoch}] Learning rate: {lr}\")\n",
    "\n",
    "    if scheduler:\n",
    "        checkpoint[\"scheduler_state_dict\"] = scheduler.state_dict()\n",
    "        patience = getattr(scheduler, \"patience\", \"N/A\")\n",
    "        logger.info(f\"[Epoch {epoch}] Scheduler patience: {patience}\")\n",
    "\n",
    "    torch.save(checkpoint, save_path)\n",
    "    logger.info(f\" Intermediate checkpoint saved to: {save_path}\")\n",
    "    return save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea448078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 加载处理后的数据\n",
    "X = np.load(\"X.npy\")\n",
    "y = np.load(\"y.npy\")\n",
    "\n",
    "print(\"X shape:\", X.shape)  # 输出 X 的形状\n",
    "print(\"y shape:\", y.shape)  # 输出 y 的形状\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def create_data_loaders(X, y, train_ratio=0.8, batch_size=32, shuffle_train=True, shuffle_val=False):\n",
    "    dataset = WeatherDataset(X, y)\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_train)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle_val)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader, val_loader = create_data_loaders(X, y, train_ratio=0.8, batch_size=32, shuffle_train=True, shuffle_val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f066a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# from your_model_file import BrainInspiredNetV2_Full, BrainInspiredNetV2_NoHormone, BrainInspiredNetV2_NoIndirect, BrainInspiredNetV2_NoTemporal, BrainInspiredNetV2_SimpleFC\n",
    "\n",
    "model_dict = {\n",
    "    \"Full\": BrainInspiredNetV2,  # 你现在的完整模型类\n",
    "    \"NoHormone\": BrainInspiredNetV2_NoHormone,\n",
    "    \"NoIndirect\": BrainInspiredNetV2_NoIndirect,\n",
    "    \"NoTemporal\": BrainInspiredNetV2_NoTemporal,\n",
    "    \"SimpleFC\": BrainInspiredNetV2_SimpleFC,\n",
    "}\n",
    "\n",
    "\n",
    "# 配置日志记录器\n",
    "def setup_logger(log_path=None, level=logging.INFO):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level)\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    if log_path:\n",
    "        file_handler = logging.FileHandler(log_path)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "# 设置日志路径\n",
    "log_path = \"training.log\"\n",
    "logger = setup_logger(log_path=log_path)\n",
    "\n",
    "# 检查设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# 你可以通过修改这里的 key 来选择消融模型\n",
    "selected_model_key = \"NoHormone\"  # 比如改成 \"Full\", \"NoIndirect\" 等\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 加载保存的 X\n",
    "X = np.load(\"X.npy\")\n",
    "print(\"X shape:\", X.shape)  # 输出 X 的形状\n",
    "\n",
    "# 1. 实例化模型\n",
    "model_class = model_dict[selected_model_key]\n",
    "model = model_class(input_dim=X.shape[2]).to(device)\n",
    "\n",
    "# 2. 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 3. 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 4. 定义 Trainer 实例\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    max_epochs=15,\n",
    "    patience=5,\n",
    "    lr_scheduler_patience=3,\n",
    "    lr_scheduler_factor=0.5,\n",
    "    save_path=\"best_model.pth\",\n",
    "    log_path=log_path,\n",
    "    custom_callbacks=[save_intermediate_model],\n",
    "    use_mask=False,\n",
    "    save_state_dict=True\n",
    ")\n",
    "\n",
    "# 5. 开始训练\n",
    "start_time = time.time()  # 记录训练开始时间\n",
    "training_results = trainer.train()\n",
    "end_time = time.time()  # 记录训练结束时间\n",
    "training_duration = end_time - start_time  # 计算训练总耗时\n",
    "\n",
    "# 6. 打印训练结果\n",
    "logger.info(f\"Starting training for model: {selected_model_key}\")\n",
    "start_time = time.time()\n",
    "training_results = trainer.train()\n",
    "end_time = time.time()\n",
    "training_duration = end_time - start_time\n",
    "logger.info(\"Training completed.\")\n",
    "logger.info(f\"Training duration: {training_duration / 60:.2f} minutes\")\n",
    "logger.info(f\"Train losses: {training_results['train_losses']}\")\n",
    "logger.info(f\"Validation losses: {training_results['val_losses']}\")\n",
    "logger.info(f\"Best epoch: {training_results['best_epoch']}\")\n",
    "logger.info(f\"Best validation loss: {training_results['best_loss']}\")\n",
    "\n",
    "\n",
    "# 7. 绘制训练曲线\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(training_results['train_losses'], label='Train Loss')\n",
    "plt.plot(training_results['val_losses'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training and Validation Losses - Model: {selected_model_key}')\n",
    "plt.legend()\n",
    "plt.savefig(f'training_curve_{selected_model_key}.png')  # 每个模型保存不同图\n",
    "plt.show()\n",
    "\n",
    "# 8. 自动保存运行配置\n",
    "# 创建一个字典来保存训练过程中的关键配置和结果\n",
    "config = {\n",
    "    \"model_name\": selected_model_key,  # 记录当前消融版本名字\n",
    "    \"input_dim\": X.shape[2],  # 输入维度\n",
    "    \"max_epochs\": 15,  # 最大训练轮数\n",
    "    \"patience\": 5,  # 早停机制的耐心值\n",
    "    \"lr_scheduler_patience\": 3,  # 学习率调度器的耐心值\n",
    "    \"lr_scheduler_factor\": 0.5,  # 学习率调度器的衰减因子\n",
    "    \"batch_size\": 8,  # 训练数据的批次大小\n",
    "    \"learning_rate\": optimizer.param_groups[0]['lr'],  # 初始学习率\n",
    "    \"device\": str(device),  # 训练使用的设备（CPU 或 GPU）\n",
    "    \"training_duration\": training_duration,  # 训练总耗时（秒）\n",
    "    \"best_epoch\": training_results['best_epoch'],  # 最佳训练轮次\n",
    "    \"best_loss\": training_results['best_loss'],  # 最佳验证损失\n",
    "    \"save_path\": \"best_model.pth\",  # 最佳模型保存路径\n",
    "    \"log_path\": log_path  # 日志文件路径\n",
    "}\n",
    "\n",
    "# 定义保存配置文件的路径\n",
    "config_path = \"training_config.json\"\n",
    "\n",
    "# 将配置字典保存为 JSON 文件\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=4)  # 使用缩进格式化 JSON 文件\n",
    "\n",
    "# 记录到日志\n",
    "logger.info(f\"Training configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d8a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型设置为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 初始化预测值和真实值的列表\n",
    "preds, actuals = [], []\n",
    "\n",
    "# 禁用梯度计算（推理阶段不需要计算梯度，节省计算资源和内存）\n",
    "with torch.no_grad():\n",
    "    # 遍历验证数据加载器中的每个批次\n",
    "    for x_batch, y_batch in val_loader:\n",
    "        # 将输入数据移动到指定设备（CPU或GPU）\n",
    "        x_batch = x_batch.to(device)\n",
    "        # 前向传播，获取预测结果（忽略激素状态）\n",
    "        pred, _ = model(x_batch)\n",
    "        # 将预测结果从张量转换为 NumPy 数组，并添加到预测值列表中\n",
    "        preds.extend(pred.cpu().numpy())\n",
    "        # 将真实值从张量转换为 NumPy 数组，并添加到真实值列表中\n",
    "        actuals.extend(y_batch.numpy())\n",
    "\n",
    "# 绘制真实值和预测值的对比图\n",
    "plt.plot(actuals, label=\"True\")  # 绘制真实值曲线，标签为 \"True\"\n",
    "plt.plot(preds, label=\"Predicted\")  # 绘制预测值曲线，标签为 \"Predicted\"\n",
    "plt.legend()  # 显示图例\n",
    "plt.title(\"Prediction on Validation Set\")  # 设置图表标题\n",
    "plt.show()  # 显示图表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(actuals, preds)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4707adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c94d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(actuals, preds)\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 sklearn.metrics 中的 mean_squared_error 函数\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 计算均方误差（MSE）\n",
    "mse = mean_squared_error(actuals, preds)  # 使用 actuals 和 preds 计算 MSE\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")  # 打印 MSE，保留 4 位小数\n",
    "\n",
    "# 导入 numpy 库\n",
    "import numpy as np\n",
    "\n",
    "# 计算均方根误差（RMSE）\n",
    "rmse = np.sqrt(mse)  # 对 MSE 取平方根得到 RMSE\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")  # 打印 RMSE，保留 4 位小数\n",
    "\n",
    "# 导入 sklearn.metrics 中的 r2_score 函数\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 计算决定系数（R² Score）\n",
    "r2 = r2_score(actuals, preds)  # 使用 actuals 和 preds 计算 R² Score\n",
    "print(f\"R² Score: {r2:.4f}\")  # 打印 R² Score，保留 4 位小数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422db3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制实际值与预测值的对比图\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actuals, label=\"True Values\", color=\"blue\")\n",
    "plt.plot(preds, label=\"Predicted Values\", color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Meteorological Values\")\n",
    "plt.title(\"Actual vs Predicted Meteorological Data\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46677b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# ========== 1. 加载模型 ==========\n",
    "model = BrainInspiredNetV2(input_dim=X.shape[2]).to(device)\n",
    "model.load_state_dict(torch.load(\"best_model_20250605_195543.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# ========== 2. 加载归一化器 ==========\n",
    "y_scaler = joblib.load(\"y_scaler.pkl\")\n",
    "\n",
    "# ========== 3. 定义反归一化函数 ==========\n",
    "def inverse_transform_target(scaler, data):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data = data.detach().cpu().numpy()\n",
    "    data = data.reshape(-1, 1)\n",
    "    return scaler.inverse_transform(data).flatten()\n",
    "\n",
    "# ========== 4. 验证模型 ==========\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs = outputs[0]\n",
    "\n",
    "        all_preds.append(outputs.cpu())\n",
    "        all_targets.append(targets.cpu())\n",
    "\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "# ========== 5. 反归一化 ==========\n",
    "y_pred_inv = inverse_transform_target(y_scaler, all_preds)\n",
    "y_true_inv = inverse_transform_target(y_scaler, all_targets)\n",
    "\n",
    "# ========== 6. 计算指标 ==========\n",
    "rmse = mean_squared_error(y_true_inv, y_pred_inv, squared=False)\n",
    "mae = mean_absolute_error(y_true_inv, y_pred_inv)\n",
    "r2 = r2_score(y_true_inv, y_pred_inv)\n",
    "\n",
    "print(\"【反归一化后验证集评估】\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE : {mae:.4f}\")\n",
    "print(f\"R²  : {r2:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 7. 可视化预测效果 ==========\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(y_true_inv[:20000], label=\"True\")\n",
    "plt.plot(y_pred_inv[:20000], label=\"Predicted\")\n",
    "plt.title(\"True vs Predicted (First 500 samples)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"prediction_vs_true.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_118_python310_jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
